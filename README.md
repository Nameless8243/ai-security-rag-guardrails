# ğŸ›¡ï¸ AI Security Lab â€“ RAG Poisoning, Drift Detection & Guardrails

This project is an educational AI Security lab demonstrating how to build a **secure Retrieval-Augmented Generation (RAG)** pipeline with:

- Document hashing & ingestion audit logs  
- Outlier detection for poisoned embeddings  
- Retriever drift monitoring  
- Context guardrails  
- LLM-based mutation detection  
- Strict policy enforcement from a single â€œofficialâ€ document  

It is intentionally simplified to help beginners understand real-world AI security concepts.

---

## ğŸ“ Project Structure

```
AI_SECURITY_LAB/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ai_security_overview.txt      # official policy docs
â”‚   â”œâ”€â”€ data_protection.txt
â”‚   â”œâ”€â”€ encryption_policy.txt
â”‚   â”œâ”€â”€ iam_rules.txt
â”‚   â”œâ”€â”€ incident_response.txt
â”‚   â”œâ”€â”€ good_policy.txt               # authoritative security policy
â”‚   â””â”€â”€ poisoned_policy.txt           # deliberately malicious policy
â”‚
â”œâ”€â”€ audit.py                          # append-only JSONL logging
â”œâ”€â”€ baseline_embedding.py             # baseline vector for drift detection
â”œâ”€â”€ context_guard.py                  # context filters + embedding drift check
â”œâ”€â”€ detect_poisoning.py               # embedding outlier detector
â”œâ”€â”€ drift.py                          # retriever drift monitor
â”œâ”€â”€ ingest.py                         # policy ingestion + hashing + split
â”œâ”€â”€ mutation_detector.py              # LLM-based mutation checking (WARN mode)
â”œâ”€â”€ rag_query.py                      # main RAG + guardrail pipeline
â”œâ”€â”€ utils.py                          # cosine similarity helper
â””â”€â”€ requirements.txt
```

---

## âœ”ï¸ Prerequisites

- Python **3.11+** recommended  
- [**Ollama**](https://ollama.com/) installed and running  
- A local model pulled, for example:

```bash
ollama pull mistral:7b
```

---

## âš™ï¸ Setup

```bash
python -m venv .venv
source .venv/bin/activate       # Linux/macOS
# .venv\Scripts\activate        # Windows

pip install -r requirements.txt
```

---

## â–¶ï¸ Running the Lab

### **1. Build the baseline embedding** (from the official policy)

```bash
python baseline_embedding.py
```

---

### **2. Ingest all policy documents into Chroma**

```bash
python ingest.py
```

The script will:

- compute SHA-256 hash for each document  
- write entries into `audit_log.jsonl`  
- split, embed, and store documents in `chroma_db/`  


---

### **3. Run the secure RAG pipeline with all guardrails**

```bash
python rag_query.py
```

You will see:

- Retrieved context (including the malicious policy)  
- Retriever drift monitoring  
- Context guardrail warnings  
- Mutation detector warnings  
- Final secured LLM answer (preferring the strict policy)  

---

## ğŸ” Security Modules Overview

### **âœ” Document Hashing & Audit Log**
- SHA-256 hash for every document
- Append-only JSONL audit trail

### **âœ” Embedding Outlier Detection**
Detects poisoned or anomalous embeddings using Z-score thresholds.

### **âœ” Retriever Drift Detection**
Flags:
- dominant documents (e.g., 95%+ retrieval rate)
- sudden new documents

### **âœ” Context Guardrails**
Blocks:
- jailbreak-style instructions  
- policy-breaking language  
- embedding drift vs baseline

### **âœ” LLM Mutation Detection**
Uses a local LLM to detect:
- rewritten policy content  
- harmful â€œexceptionsâ€
- permissions that contradict the official policy

---

## âš ï¸ Disclaimer

This project is **for educational purposes only**.  
It intentionally simplifies core AI security concepts:

- RAG poisoning  
- Embedding integrity checks  
- Guardrails  
- Drift monitoring  
- LLM mutation analysis  

**Do not reuse this code in production systems.**

---

## ğŸ“„ License

MIT License  
Copyright Â© 2025  

